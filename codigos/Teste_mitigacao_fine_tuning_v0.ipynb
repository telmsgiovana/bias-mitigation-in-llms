{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Xsh6oIVOJnM"
      },
      "outputs": [],
      "source": [
        "!pip install -q openai tqdm\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2_mYfasOO8Q"
      },
      "outputs": [],
      "source": [
        "# Modelos a testar (via OpenRouter)\n",
        "modelos = [\n",
        "    \"qwen/qwen-2.5-7b-instruct\"  \n",
        "    #\"mistralai/mistral-7b-instruct\",\n",
        "    #\"google/gemma-3-4b-it\"\n",
        "   # \"meta-llama/llama-3.1-8b-instruct\",\n",
        "]\n",
        "\n",
        "# Estratégias de mitigação via prompt\n",
        "prompts_mitigacao = {\n",
        "    \"aware\": (\n",
        "        \"When analyzing information, remain aware of confirmation bias — \"\n",
        "        \"the tendency to favor data that supports prior beliefs while ignoring conflicting evidence. \"\n",
        "        \"Actively seek out contradictory information and incorporate it into your reasoning. \"\n",
        "        \"Strive for a balanced and evidence-based response.\"\n",
        "        \"Then give your answer in a direct way.\"\n",
        "    ),\n",
        "     \"autocritica_simulada\": (\n",
        "        \"Answer the question first. Then, reflect on your reasoning: \"\n",
        "        \"Was any key evidence overlooked? Could your response have been influenced by confirmation bias? \"\n",
        "        \"Revise your answer and state your impressions in an direct way.\"\n",
        "    ),\n",
        "    \"multiplas_fontes\": (\n",
        "        \"Ensure your answer is informed by diverse sources, not just those that align with initial assumptions. \"\n",
        "        \"Actively seek out credible perspectives that challenge your viewpoint to increase objectivity and accuracy.\"\n",
        "        \"Give your answer in a direct way.\"\n",
        "    ),\n",
        "    \"protocolo_verificacao\": (\n",
        "        \"Before forming a conclusion, apply this verification protocol:\\n\"\n",
        "        \"(1) Search for disconfirming evidence.\\n\"\n",
        "        \"(2) Consider the strongest counterarguments.\\n\"\n",
        "        \"(3) Check for cherry-picked data.\\n\"\n",
        "        \"(4) Review sources that challenge your assumptions.\\n\"\n",
        "        \"(5) Identify what evidence would change your mind.\\n\"\n",
        "        \"(6) Give your answer in a direct way .\\n\"\n",
        "\n",
        "    )\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4Ix8YBvOZJu"
      },
      "outputs": [],
      "source": [
        "def montar_prompt(instrucao, pergunta):\n",
        "    return f\"{instrucao}\\n\\nQuestion: {pergunta}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQPlBtKcOcO8"
      },
      "outputs": [],
      "source": [
        "def consultar_openrouter(prompt, chave_api, model):\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {chave_api}\",\n",
        "        \"HTTP-Referer\": \"https://chat.openai.com/\",\n",
        "        \"X-Title\": \"Viés Prompt Mitigation\"\n",
        "    }\n",
        "\n",
        "    body = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=body)\n",
        "    response.raise_for_status()\n",
        "    return response.json()['choices'][0]['message']['content']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDZSTdqNOfu8"
      },
      "outputs": [],
      "source": [
        "def testar_mitigacao_prompt(arquivo_perguntas, chave_api):\n",
        "    perguntas_df = pd.read_csv(arquivo_perguntas)\n",
        "\n",
        "    for modelo in tqdm(modelos, desc=\"Modelos\"):\n",
        "        resultados = []\n",
        "\n",
        "        for _, linha in tqdm(perguntas_df.iterrows(), total=perguntas_df.shape[0], desc=\"Perguntas\", leave=False):\n",
        "            pergunta = linha[\"pergunta\"]\n",
        "            pergunta_id = linha[\"id\"]\n",
        "\n",
        "            linha_resultado = {\n",
        "                \"id\": pergunta_id,\n",
        "                \"pergunta\": pergunta,\n",
        "                \"modelo\": modelo,\n",
        "                \"resposta_aware\": \"\",\n",
        "                \"resposta_autocritica_simulada\": \"\",\n",
        "                \"resposta_multiplas_fontes\": \"\",\n",
        "                 \"resposta_protocolo_verificacao\": \"\"\n",
        "            }\n",
        "\n",
        "            for chave_prompt, instrucao in prompts_mitigacao.items():\n",
        "                prompt_final = montar_prompt(instrucao, pergunta)\n",
        "\n",
        "                try:\n",
        "                    resposta = consultar_openrouter(prompt_final, chave_api, model=modelo)\n",
        "                    time.sleep(20)  # evita erro 429\n",
        "                except Exception as e:\n",
        "                    resposta = f\"Erro: {str(e)}\"\n",
        "\n",
        "                linha_resultado[f\"resposta_{chave_prompt}\"] = resposta\n",
        "\n",
        "            resultados.append(linha_resultado)\n",
        "\n",
        "        df_resultado = pd.DataFrame(resultados)\n",
        "        nome_arquivo = f\"respostas_prompt_mitigacao_{modelo.replace('/', '_').replace(':', '-')}.csv\"\n",
        "        df_resultado.to_csv(nome_arquivo, index=False)\n",
        "        print(f\"✅ Resultados salvos em: {nome_arquivo}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPM9O6_HO7SH",
        "outputId": "b860f861-3b04-4470-d264-2a87b2c3c487"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Modelos:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Perguntas:   0%|          | 0/52 [00:00<?, ?it/s]\u001b[A\n",
            "Perguntas:   2%|▏         | 1/52 [01:39<1:24:32, 99.45s/it]\u001b[A\n",
            "Perguntas:   4%|▍         | 2/52 [03:15<1:21:13, 97.48s/it]\u001b[A\n",
            "Perguntas:   6%|▌         | 3/52 [04:46<1:17:15, 94.61s/it]\u001b[A\n",
            "Perguntas:   8%|▊         | 4/52 [06:23<1:16:12, 95.27s/it]\u001b[A\n",
            "Perguntas:  10%|▉         | 5/52 [08:04<1:16:27, 97.61s/it]\u001b[A\n",
            "Perguntas:  12%|█▏        | 6/52 [09:38<1:13:55, 96.42s/it]\u001b[A\n",
            "Perguntas:  13%|█▎        | 7/52 [11:12<1:11:38, 95.52s/it]\u001b[A\n",
            "Perguntas:  15%|█▌        | 8/52 [12:46<1:09:37, 94.94s/it]\u001b[A\n",
            "Perguntas:  17%|█▋        | 9/52 [14:20<1:07:50, 94.67s/it]\u001b[A\n",
            "Perguntas:  19%|█▉        | 10/52 [15:53<1:05:58, 94.24s/it]\u001b[A\n",
            "Perguntas:  21%|██        | 11/52 [17:27<1:04:18, 94.11s/it]\u001b[A\n",
            "Perguntas:  23%|██▎       | 12/52 [19:04<1:03:15, 94.89s/it]\u001b[A\n",
            "Perguntas:  25%|██▌       | 13/52 [20:39<1:01:41, 94.90s/it]\u001b[A\n",
            "Perguntas:  27%|██▋       | 14/52 [22:17<1:00:45, 95.92s/it]\u001b[A\n",
            "Perguntas:  29%|██▉       | 15/52 [23:55<59:30, 96.51s/it]  \u001b[A\n",
            "Perguntas:  31%|███       | 16/52 [25:35<58:34, 97.62s/it]\u001b[A\n",
            "Perguntas:  33%|███▎      | 17/52 [27:08<56:08, 96.24s/it]\u001b[A\n",
            "Perguntas:  35%|███▍      | 18/52 [28:43<54:22, 95.96s/it]\u001b[A\n",
            "Perguntas:  37%|███▋      | 19/52 [30:22<53:16, 96.87s/it]\u001b[A\n",
            "Perguntas:  38%|███▊      | 20/52 [31:55<50:57, 95.53s/it]\u001b[A\n",
            "Perguntas:  40%|████      | 21/52 [33:25<48:29, 93.85s/it]\u001b[A\n",
            "Perguntas:  42%|████▏     | 22/52 [34:57<46:46, 93.55s/it]\u001b[A\n",
            "Perguntas:  44%|████▍     | 23/52 [36:46<47:22, 98.03s/it]\u001b[A\n",
            "Perguntas:  46%|████▌     | 24/52 [38:18<44:53, 96.18s/it]\u001b[A\n",
            "Perguntas:  48%|████▊     | 25/52 [39:54<43:16, 96.18s/it]\u001b[A\n",
            "Perguntas:  50%|█████     | 26/52 [41:28<41:23, 95.51s/it]\u001b[A\n",
            "Perguntas:  52%|█████▏    | 27/52 [43:00<39:22, 94.51s/it]\u001b[A\n",
            "Perguntas:  54%|█████▍    | 28/52 [44:39<38:23, 95.96s/it]\u001b[A\n",
            "Perguntas:  56%|█████▌    | 29/52 [46:12<36:25, 95.00s/it]\u001b[A\n",
            "Perguntas:  58%|█████▊    | 30/52 [47:46<34:43, 94.69s/it]\u001b[A\n",
            "Perguntas:  60%|█████▉    | 31/52 [49:20<33:04, 94.51s/it]\u001b[A\n",
            "Perguntas:  62%|██████▏   | 32/52 [50:55<31:30, 94.51s/it]\u001b[A\n",
            "Perguntas:  63%|██████▎   | 33/52 [52:29<29:56, 94.55s/it]\u001b[A\n",
            "Perguntas:  65%|██████▌   | 34/52 [54:02<28:09, 93.83s/it]\u001b[A\n",
            "Perguntas:  67%|██████▋   | 35/52 [55:32<26:20, 92.97s/it]\u001b[A\n",
            "Perguntas:  69%|██████▉   | 36/52 [57:12<25:18, 94.89s/it]\u001b[A\n",
            "Perguntas:  71%|███████   | 37/52 [58:47<23:43, 94.87s/it]\u001b[A\n",
            "Perguntas:  73%|███████▎  | 38/52 [1:00:23<22:12, 95.20s/it]\u001b[A\n",
            "Perguntas:  75%|███████▌  | 39/52 [1:02:00<20:44, 95.72s/it]\u001b[A\n",
            "Perguntas:  77%|███████▋  | 40/52 [1:03:34<19:05, 95.43s/it]\u001b[A\n",
            "Perguntas:  79%|███████▉  | 41/52 [1:05:06<17:18, 94.38s/it]\u001b[A\n",
            "Perguntas:  81%|████████  | 42/52 [1:06:45<15:57, 95.73s/it]\u001b[A\n",
            "Perguntas:  83%|████████▎ | 43/52 [1:08:21<14:20, 95.66s/it]\u001b[A\n",
            "Perguntas:  85%|████████▍ | 44/52 [1:09:53<12:36, 94.60s/it]\u001b[A\n",
            "Perguntas:  87%|████████▋ | 45/52 [1:11:27<11:02, 94.62s/it]\u001b[A\n",
            "Perguntas:  88%|████████▊ | 46/52 [1:13:03<09:29, 94.87s/it]\u001b[A\n",
            "Perguntas:  90%|█████████ | 47/52 [1:14:42<08:01, 96.29s/it]\u001b[A\n",
            "Perguntas:  92%|█████████▏| 48/52 [1:16:14<06:19, 94.97s/it]\u001b[A\n",
            "Perguntas:  94%|█████████▍| 49/52 [1:17:46<04:41, 93.94s/it]\u001b[A\n",
            "Perguntas:  96%|█████████▌| 50/52 [1:19:18<03:07, 93.51s/it]\u001b[A\n",
            "Perguntas:  98%|█████████▊| 51/52 [1:20:51<01:33, 93.11s/it]\u001b[A\n",
            "Perguntas: 100%|██████████| 52/52 [1:22:35<00:00, 96.51s/it]\u001b[A\n",
            "Modelos: 100%|██████████| 1/1 [1:22:35<00:00, 4955.57s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Resultados salvos em: respostas_prompt_mitigacao_qwen_qwen-2.5-7b-instruct.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 🔑 Sua chave da OpenRouter (coloque a sua aqui)\n",
        "chave_api = \"\"  # substitua pela sua chave real\n",
        "\n",
        "# 🏁 Executar o pipeline com seu arquivo\n",
        "testar_mitigacao_prompt(\"perguntas_enviesadas.csv\", chave_api)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
